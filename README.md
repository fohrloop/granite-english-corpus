# Granite English Corpus

This repository contains the character-based ngrams (unigrams, bigrams, trigrams) used for the development of the [Granite Layout](https://github.com/fohrloop/granite-layout) which are compatible with the [Keyboard Layout Optimizer](https://github.com/dariogoetz/keyboard_layout_optimizer) by Dario GÃ¶tz. The corpus was cleaned from untypical characters before creating the ngrams (see below for details).

The used  corpus is a mixture of two dataset with following weights

- 40% Leipzig (10% News, 10% Web-public com, 10% Web-public UK & 10% Wikipedia)
- 60% Reddit TLDR17

Other related repos: [granite-code-ngrams](https://github.com/fohrloop/granite-code-ngrams) and [granite-finnish-ngrams](https://github.com/fohrloop/granite-finnish-ngrams).

# How the corpus was created

## The Granite Leipzig dataset
The Granite English corpus (`ngrams/english`) is a mixture of following from the [Leipzig Corpora Collection](https://wortschatz.uni-leipzig.de/en/download)[^leipzig]:

- News 2023 1M dataset (`eng_news_2023_1M.tar.gz`)
- Web-public com 1M dataset (`eng-com_web-public_2018_1M.tar.gz`)
- Web public United Kingdow 1M dataset (`eng-uk_web-public_2018_1M.tar.gz`)
- Wikipedia 2016 1M dataset (`eng_wikipedia_2016_1M.tar.gz`)

The Granite Eglish Leipzig dataset uses only the `*-sentences.txt` file from each, containing 4M English sentences in total. The sentence numbering and the tab was removed, and all the data from the four `*-sentences.txt` was merged into one. After cleaning the data, the `ngrams` binary from the [Keyboard Layout Optimizer](https://github.com/dariogoetz/keyboard_layout_optimizer) was used to form the Granite Leipzig ngrams (`ngrams/leipzig`).


<details>
<summary>Script used to create `combined-leipzig.txt`</summary>

```python
from pathlib import Path

folder = Path(__file__).parent / "raw" / "leipzig"
outfile = Path(__file__).parent / "corpus-not-clean" / "combined-leipzig.txt"
outfile.parent.mkdir(exist_ok=True)


def process_files():
    with open(outfile, "w") as out:
        for file in folder.glob("*.txt"):
            process_file(file, out)


def process_file(filename: Path, outfile):
    print(f"Processing {filename}")
    with open(filename, "r") as file:
        for line in file:
            linedata = line.split("\t", maxsplit=1)[-1]
            print(linedata, file=outfile, end="")


if __name__ == "__main__":
    process_files()
```

</details>

## The Granite Reddit TLDR17 dataset

IThe `tldr17` dataset was created from the [Webis-TLDR-17 Corpus](https://zenodo.org/records/1043504)[^tldr17]. The corpus was created by extracting the "content" field from each of the rows, stripping whitespace from the beginning and end of each, adding single space between contents, and combining all of that into a single 5.6GB text file (`tldr17.txt`).


<details>
<summary>Script used to create `tldr17.txt`</summary>

```python
import json
from pathlib import Path

infile = Path(__file__).parent / "raw" / "corpus-webis-tldr-17.json"
outfile = Path(__file__).parent / "corpus-not-clean"  / "tldr17.txt"
outfile.parent.mkdir(exist_ok=True)


def process_file():
    with open(infile, "r") as indata, open(outfile, "w") as out:
        for line in indata:
            data = json.loads(line)
            content = data["content"].strip()
            print(content, file=out, end=" ")


if __name__ == "__main__":
    process_file()
```

</details>

## The Granite English dataset

The Granite English ngrams were created from the  `leipzig` and the `tldr17` by first normalizing the ngram files using the `normalize.py` script from the [Keyboard Layout Optimizer](https://github.com/dariogoetz/keyboard_layout_optimizer)

```
python ./scripts/ngrams/normalize.py /somepath/ngrams/<corpusname>
```

and then merging the ngram files with weighting using the `ngram_merge` binary from the Keyboard Layout Optimizer:

```
â¯ ./target/release/ngram_merge english/ngrams/english english/ngrams/leipzig:0.4 english/ngrams/tldr17:0.6
```

The used version of the Keyboard Layout Optimizer is specified by commit [f93bd06](https://github.com/dariogoetz/keyboard_layout_optimizer/commit/f93bd06820790ae746fbe66445bdf4427260fd31).

## Cleaning the datasets

Before creating the ngrams, only following ASCII alphanumerics (62 characters):
```
qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM1234567890
```

and following punctuation and special characters (33 characters):

```
,.!?;:_'"^~#%&/\()[]{}<>=+-*`@$â‚¬|
```

were accepted to the ngrams. The conversion script below was used to either remove or replace characters which belong to the following set:

```
Â¡Â£Â¤Â§Â«Â­Â®Â°Â²Â³Â´Â¶Â·Â¹Â»Â½Â¾Â¿ÃÃƒÃ…Ã†Ã‰ÃŒÃ“Ã—Ã˜ÃŸÃ Ã¡Ã¢Ã£Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¸ÃºÃ»Ã¼Ã½ÄÄƒÄ‡ÄÄ«Å‚Å‹ÅÅ Å¡Å«Å½Å¾É™ÍœÍ¡Î±Î²ÎµÎ·Î¹ÎºÎ»Î¼Î½Î¿Ï€ÏÏ‚ÏƒÏ„Ï…ĞĞ©Ğ°Ğ²Ğ´ĞµĞ¸ĞºĞ»Ğ¼Ğ½Ğ¾Ñ€ÑÑ‚Ø£Ø§Ø¨Ø©ØªØ®Ø¯Ø±Ø³Ø¹Ù‚Ù„Ù…Ù†Ù‡ÙˆÙŠà¸™à¸£à¸¥à¸­à¸²à¹€â€‹â€’â€“â€”â€•â€˜â€™â€œâ€â€¢â€¦â‚¹â„¢â†’âˆ’â”€â”‚â”†â”Œâ”â””â”˜â”¬â”´â•â•â•¡â•ªâ™ªâ™«æœˆè¯­ğ¤«
```


<details>
<summary>Corpora cleanup script</summary>

This is the cleanup script which was used to clean the English, Code and Finnish corpora. 

```python
from pathlib import Path

root = Path(__file__).parent.parent


TYPABLE_CHARS = "qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM1234567890"
TYPABLE_CHARS += r""",.!?;:_'"^~#%&/\()[]{}<>=+-*`@$â‚¬|"""
TYPABLE_CHARS += " \t\n"
ALLOWED_CHARACTERS = set(TYPABLE_CHARS)
ALLOWED_CHARACTERS_FINNISH = ALLOWED_CHARACTERS | set("Ã¤Ã¶Ã„Ã–")

replacements_finnish = {
    "Â¹": "1",
    "Â²": "2",
    "Â³": "3",
    "Â½": "1/2",
    "Â¾": "3/4",
    "Ã": "A",
    "Ãƒ": "A",
    "Ã…": "A",
    "Ã†": "AE",
    "Ã‰": "E",
    "ÃŒ": "I",
    "Ã“": "O",
    "Ã—": "x",
    "Ã˜": "Ã–",
    "ÃŸ": "ss",
    "Ã ": "a",
    "Ã¡": "a",
    "Ã¢": "a",
    "Ã£": "a",
    "Ã¥": "a",
    "Ã¦": "ae",
    "Ã§": "c",
    "Ã¨": "e",
    "Ã©": "e",
    "Ãª": "e",
    "Ã«": "e",
    "Ã¬": "i",
    "Ã­": "i",
    "Ã®": "i",
    "Ã¯": "i",
    "Ã°": "d",
    "Ã±": "n",
    "Ã²": "o",
    "Ã³": "o",
    "Ã´": "o",
    "Ãµ": "o",
    "Ã¸": "Ã¶",
    "Ãº": "u",
    "Ã»": "u",
    "Ã¼": "u",
    "Ã½": "y",
    "Ä": "a",
    "Äƒ": "a",
    "Ä‡": "c",
    "Ä": "c",
    "Ä«": "i",
    "Å‚": "l",
    "Å‹": "NG",
    "Å": "o",
    "Å ": "S",
    "Å¡": "s",
    "Å«": "u",
    "Å½": "Z",
    "Å¾": "z",
    "Î±": "a",
    "Î²": "b",
    "Îµ": "e",
    "Î·": "e",
    "Î¹": "i",
    "Îº": "k",
    "Î»": "l",
    "Î¼": "m",
    "Î½": "n",
    "Î¿": "o",
    "Ï€": "p",
    "Ï": "r",
    "Ï‚": "s",
    "Ïƒ": "s",
    "Ï„": "t",
    "Ï…": "u",
    "Ğ": "A",
    "Ğ©": "Shch",
    "Ğ°": "a",
    "Ğ²": "v",
    "Ğ´": "d",
    "Ğµ": "e",
    "Ğ¸": "i",
    "Ğº": "k",
    "Ğ»": "l",
    "Ğ¼": "m",
    "Ğ½": "n",
    "Ğ¾": "o",
    "Ñ€": "r",
    "Ñ": "s",
    "Ñ‚": "t",
    "â€’": "-",
    "â€“": "-",
    "â€”": "--",
    "â€•": "--",
    "âˆ’": "-",
    "â”€": "-",
    "â€˜": "'",
    "â€™": "'",
    "Â´": "'",
    "â€œ": '"',
    "â€": '"',
    "Â«": '"',
    "Â»": '"',
    "â€¢": "*",
    "Â·": "*",
    "â€¦": "...",
    "â„¢": "(tm)",
    "Â­Â®": "(r)",
}
replacements = replacements_finnish.copy()
replacements["Ã˜"] = "O"
replacements["Ã¸"] = "o"


def process_file(input_path, output_path, replacements, allowed_chars):
    chunk_size = 50 * 1024 * 1024  # 50 MB chunk size
    with open(input_path, "r", encoding="utf-8") as infile, open(
        output_path, "w", encoding="utf-8"
    ) as outfile:
        while True:
            chunk = infile.read(chunk_size)
            if not chunk:
                break  # End of file

            for old_str, new_str in replacements.items():
                chunk = chunk.replace(old_str, new_str)
            chunk = "".join(char for char in chunk if char in allowed_chars)
            outfile.write(chunk)


def cleanup_folder(
    folder: Path,
    folder_out: Path,
    allowed_characters: set[str],
    used_replacements: dict[str, str],
):
    folder_out.mkdir(exist_ok=True)
    for file in folder.glob("*.txt"):
        file_out = folder_out / file.name
        print(f"Processing {file} -> {file_out}")
        process_file(file, file_out, used_replacements, allowed_characters)


if __name__ == "__main__":
    import time

    start = time.time()
    for lang in ("english", "code", "finnish"):
        langfolder = root / lang
        allowed_characters = (
            ALLOWED_CHARACTERS_FINNISH if lang == "finnish" else ALLOWED_CHARACTERS
        )
        used_replacements = replacements_finnish if lang == "finnish" else replacements
        cleanup_folder(
            langfolder / "corpus-not-clean",
            langfolder / "corpus-clean",
            allowed_characters,
            used_replacements,
        )
    print("Done in", time.time() - start, "seconds")
```
</details>



[^leipzig]: D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages. In: Proceedings of the 8th International Language Resources and Evaluation (LREC'12), 2012
[^tldr17]: Syed, S., Voelske, M., Potthast, M., & Stein, B. (2017). Webis-TLDR-17 Corpus [Data set]. EMNLP 2017 Workshop on New Frontiers in Summarization (EMNLP 2017). Zenodo. https://doi.org/10.5281/zenodo.1043504